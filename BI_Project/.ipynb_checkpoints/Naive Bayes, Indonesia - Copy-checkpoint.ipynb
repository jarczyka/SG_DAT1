{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Double check the map command -- may not be giving you what you need! \n",
    "# Use sci-kit's multinoulli naive bayes library as reference --> that will help determine whether it's the data or the code \n",
    "# If nothing works, send Misrab a link to my repo and he'll take a look \n",
    "\n",
    "# Check my overarching methodology -- am I doing this the right way? \n",
    "# Consolidating documents, process steps, priors, \n",
    "\n",
    "# Regardless of whether I use Indonesia or the Philippines, it looks like the sample will be skewed \n",
    "# Is there a standard method for dealing with skewed samples? (In this case, a single set that accounts for 65-75% of the total)\n",
    "# It seems like the fundamental issue here is likely to be document length \n",
    "\n",
    "# Not sure which prior to use... Also, it looks like I may be putting the prior into the calculations at the wrong point \n",
    "# By the time we get to the end, the prior is too small to make a difference (by several orders of magnitude). Is this right? \n",
    "\n",
    "# Still to review: \n",
    "# NLP for finance \n",
    "# Masters thesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary Date</th>\n",
       "      <th>Summary file number</th>\n",
       "      <th>Decreased</th>\n",
       "      <th>Same</th>\n",
       "      <th>Increased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/3/2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/20/2009</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/20/2009</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/13/2009</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/9/2009</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Summary Date  Summary file number  Decreased  Same  Increased\n",
       "0     2/3/2009                    1          1     0          0\n",
       "1    3/20/2009                    2          1     0          0\n",
       "2    4/20/2009                    3          1     0          0\n",
       "3    5/13/2009                    4          1     0          0\n",
       "4     6/9/2009                    5          1     0          0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import master matrix for testing and examine \n",
    "\n",
    "master = pd.read_csv('C:/Users/ajarczyk/Dropbox/Programming/BI_Project/Master matrix.csv')\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decreased    10\n",
       "Same         67\n",
       "Increased     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine breakdown for the total sample \n",
    "\n",
    "master[['Decreased', 'Same', 'Increased']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decreased     8\n",
       "Same         54\n",
       "Increased     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide the data set for training, testing and examine breakdown \n",
    "# Is there a good way to make this replicable? I've tried using random.setstate() but the following indices continue to change: \n",
    "#test.index\n",
    "#train.index\n",
    "\n",
    "training, testing = train_test_split(master, test_size = 0.2)\n",
    "training[['Decreased', 'Same', 'Increased']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.69314718055994529"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the priors of the sample (used to normalize results across uneven samples)\n",
    "\n",
    "# Training prior \n",
    "training_prob_s = float(training['Same'].sum()) / len(training['Same'])\n",
    "training_prob_m = float(training['Decreased'].sum() + training['Increased'].sum()) / len(training['Same'])\n",
    "training_prior = np.log(training_prob_s / training_prob_m)\n",
    "\n",
    "# Sample prior\n",
    "sample_prob_s = float(master['Same'].sum()) / len(master['Same'])\n",
    "sample_prob_m = float(master['Decreased'].sum() + master['Increased'].sum()) / len(master['Same'])\n",
    "sample_prior = np.log(sample_prob_s / sample_prob_m)\n",
    "sample_prior\n",
    "\n",
    "# Uniform prior (1/3, 1/3, 1/3)\n",
    "uniform_prior = np.log((1.0/3) / float(2.0/3))\n",
    "uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the training set into two groups for training: \n",
    "# A 'steady' group for when interest rates remain steady \n",
    "# A 'move' group for when interest rates move (increase or decrease)\n",
    "\n",
    "training_steady = training[training.Same==1]\n",
    "training_move = training[training.Same==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open training files for each training group and combine into single text files\n",
    "# Feed python the appropriate character set for decoding \n",
    "\n",
    "raw_steady = str()\n",
    "for i in (training_steady.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read()\n",
    "    raw_steady += temp\n",
    "raw_steady = raw_steady.decode('utf_16')\n",
    "\n",
    "raw_move = str()\n",
    "for i in (training_move.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read()\n",
    "    raw_move += temp\n",
    "raw_move = raw_move.decode('utf_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate the individual words in the documents\n",
    "\n",
    "tokens_steady = nltk.word_tokenize(raw_steady)\n",
    "tokens_move = nltk.word_tokenize(raw_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make all words lower case\n",
    "# Remove punctuation, \\n's, and numbers \n",
    "\n",
    "tokens_steady = [word.lower() for word in tokens_steady if word.isalpha()]\n",
    "tokens_move = [word.lower() for word in tokens_move if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stopwards\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens_steady = [word for word in tokens_steady if word not in stopwords]\n",
    "tokens_move = [word for word in tokens_move if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each word \n",
    "\n",
    "fdist_steady = nltk.FreqDist(tokens_steady)\n",
    "fdist_move = nltk.FreqDist(tokens_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an index for each training set \n",
    "\n",
    "vocabulary_steady = fdist_steady.keys()\n",
    "vocabulary_move = fdist_move.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a data table for the word count \n",
    "\n",
    "df_steady = pd.Series(fdist_steady, index=vocabulary_steady)\n",
    "df_move = pd.Series(fdist_move, index=vocabulary_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize both data tables to account for differing sample sizes (appraoch 1: see data science for business)\n",
    "\n",
    "#df_steady = df_steady / float(df_steady.sum())\n",
    "#df_move = df_move / float(df_move.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize both data tables to account for differing sample sizes (appraoch 2: see intro to information retrieval)\n",
    "\n",
    "#a = 0.5\n",
    "#df_steady = a + (1 - a)*(df_steady / float(df_steady.max()))\n",
    "#df_move = a + (1 - a)*(df_move / float(df_move.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add one to every count to deal with rare words via \"additive smoothing\"\n",
    "\n",
    "df_steady = df_steady + 1 \n",
    "df_move = df_move + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the sample probability for each word \n",
    "\n",
    "df_steady = df_steady / sum(df_steady)\n",
    "df_move = df_move / sum(df_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log the values to account for floating point underflow \n",
    "\n",
    "df_steady = np.log(df_steady)\n",
    "df_move = np.log(df_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2606.000000\n",
       "mean       -8.850639\n",
       "std         1.165818\n",
       "min        -9.862770\n",
       "25%        -9.862770\n",
       "50%        -9.169623\n",
       "75%        -8.253332\n",
       "max        -4.167355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the training sets (1)\n",
    "\n",
    "df_steady.sort_values(inplace=True)\n",
    "df_steady.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1436.000000\n",
       "mean       -7.727416\n",
       "std         0.779680\n",
       "min        -8.323244\n",
       "25%        -8.323244\n",
       "50%        -7.917779\n",
       "75%        -7.406954\n",
       "max        -4.111117\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the training sets (2)\n",
    "\n",
    "df_move.sort_values(inplace=True)\n",
    "df_move.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Steady for doc 44 is -2960.06573891\n",
      "P-Move for doc 44 is -2867.25105349\n",
      "After doc 44 interest rates should move.\n",
      "P-Steady for doc 76 is -4159.43741969\n",
      "P-Move for doc 76 is -3820.94646084\n",
      "After doc 76 interest rates should move.\n",
      "P-Steady for doc 61 is -4711.38128698\n",
      "P-Move for doc 61 is -4105.57773683\n",
      "After doc 61 interest rates should move.\n",
      "P-Steady for doc 57 is -3859.08792033\n",
      "P-Move for doc 57 is -3467.90665436\n",
      "After doc 57 interest rates should move.\n",
      "P-Steady for doc 6 is -6080.17601057\n",
      "P-Move for doc 6 is -5639.1712337\n",
      "After doc 6 interest rates should move.\n",
      "P-Steady for doc 58 is -3793.72515159\n",
      "P-Move for doc 58 is -3529.16161629\n",
      "After doc 58 interest rates should move.\n",
      "P-Steady for doc 66 is -4032.87730748\n",
      "P-Move for doc 66 is -3527.51913459\n",
      "After doc 66 interest rates should move.\n",
      "P-Steady for doc 48 is -4489.17150569\n",
      "P-Move for doc 48 is -4222.52920042\n",
      "After doc 48 interest rates should move.\n",
      "P-Steady for doc 47 is -3135.51952382\n",
      "P-Move for doc 47 is -2985.35233395\n",
      "After doc 47 interest rates should move.\n",
      "P-Steady for doc 25 is -3306.88381424\n",
      "P-Move for doc 25 is -3032.87160183\n",
      "After doc 25 interest rates should move.\n",
      "P-Steady for doc 64 is -5775.17263588\n",
      "P-Move for doc 64 is -5174.77521259\n",
      "After doc 64 interest rates should move.\n",
      "P-Steady for doc 52 is -3227.09531927\n",
      "P-Move for doc 52 is -3015.61447483\n",
      "After doc 52 interest rates should move.\n",
      "P-Steady for doc 49 is -2638.74802235\n",
      "P-Move for doc 49 is -2523.65652254\n",
      "After doc 49 interest rates should move.\n",
      "P-Steady for doc 36 is -3903.24562113\n",
      "P-Move for doc 36 is -3655.65045308\n",
      "After doc 36 interest rates should move.\n",
      "P-Steady for doc 19 is -4909.67380235\n",
      "P-Move for doc 19 is -4424.32084793\n",
      "After doc 19 interest rates should move.\n",
      "P-Steady for doc 41 is -3303.71146459\n",
      "P-Move for doc 41 is -3131.5982711\n",
      "After doc 41 interest rates should move.\n",
      "P-Steady for doc 62 is -3988.70913836\n",
      "P-Move for doc 62 is -3551.74935352\n",
      "After doc 62 interest rates should move.\n"
     ]
    }
   ],
   "source": [
    "# Open files for the testing group and combine into a single text file\n",
    "# Feed python the appropriate character set for decoding \n",
    "# Separate the individual words in the testing document\n",
    "# Make all words lower case\n",
    "# Remove punctuation, \\n's, numbers, and stop words\n",
    "# Create a data frame for the testing set \n",
    "# Calculate probabilities for test set from training sets \n",
    "\n",
    "for i in (testing.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read().decode('utf_16')\n",
    "    tokens = nltk.word_tokenize(temp)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    df_testing = pd.Series(tokens)    \n",
    "    p_steady = df_testing.map(df_steady).sum() \n",
    "    p_move = df_testing.map(df_move).sum()\n",
    "    print \"P-Steady for doc \" + str(i) + \" is\", p_steady\n",
    "    print \"P-Move for doc \" + str(i) + \" is\", p_move\n",
    "    if (p_steady - p_move) > 0: \n",
    "        print \"After doc \" + str(i) + \" interest rates should be steady.\" \n",
    "    else:\n",
    "        print \"After doc \" + str(i) + \" interest rates should move.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
