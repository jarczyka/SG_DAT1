{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure which prior to use... Also, it looks like I may be putting the prior into the calculations at the wrong point \n",
    "# By the time we get to the end, the prior is too small to make a difference (by several orders of magnitude). Is this right? \n",
    "\n",
    "# Regardless of whether I use Indonesia or the Philippines, it looks like the sample will be skewed \n",
    "# Is there a standard method for dealing with skewed samples? (In this case, a single set that accounts for 65-75% of the total)\n",
    "# It seems like the fundamental issue here is likely to be document length (see Frank And Bouckaert)\n",
    "# Can I use augmented frequency to account for document length a la https://en.wikipedia.org/wiki/Tf%E2%80%93idf \n",
    "# Or can I use the constant from the Frank And Bouckaert paper (not sure if they're the same thing...)\n",
    "\n",
    "# Still to review: \n",
    "# NLP for finance \n",
    "# Masters thesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary Date</th>\n",
       "      <th>Summary file number</th>\n",
       "      <th>Decreased</th>\n",
       "      <th>Same</th>\n",
       "      <th>Increased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/3/2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/20/2009</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/20/2009</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/13/2009</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/9/2009</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Summary Date  Summary file number  Decreased  Same  Increased\n",
       "0     2/3/2009                    1          1     0          0\n",
       "1    3/20/2009                    2          1     0          0\n",
       "2    4/20/2009                    3          1     0          0\n",
       "3    5/13/2009                    4          1     0          0\n",
       "4     6/9/2009                    5          1     0          0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import master matrix for testing and examine \n",
    "\n",
    "master = pd.read_csv('C:/Users/ajarczyk/Dropbox/Programming/BI_Project/Master matrix.csv')\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decreased    10\n",
       "Same         67\n",
       "Increased     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine breakdown for the total sample \n",
    "\n",
    "master[['Decreased', 'Same', 'Increased']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decreased     6\n",
       "Same         54\n",
       "Increased     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide the data set for training, testing and examine breakdown \n",
    "# Is there a good way to make this replicable? I've tried using random.setstate() but the following indices continue to change: \n",
    "#test.index\n",
    "#train.index\n",
    "\n",
    "training, testing = train_test_split(master, test_size = 0.2)\n",
    "training[['Decreased', 'Same', 'Increased']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.69314718055994529"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the priors of the sample (used to normalize results across uneven samples)\n",
    "\n",
    "# Training prior \n",
    "training_prob_s = float(training['Same'].sum()) / len(training['Same'])\n",
    "training_prob_m = float(training['Decreased'].sum() + training['Increased'].sum()) / len(training['Same'])\n",
    "training_prior = np.log(training_prob_s / training_prob_m)\n",
    "\n",
    "# Sample prior\n",
    "sample_prob_s = float(master['Same'].sum()) / len(master['Same'])\n",
    "sample_prob_m = float(master['Decreased'].sum() + master['Increased'].sum()) / len(master['Same'])\n",
    "sample_prior = np.log(sample_prob_s / sample_prob_m)\n",
    "sample_prior\n",
    "\n",
    "# Uniform prior (1/3, 1/3, 1/3)\n",
    "uniform_prior = np.log((1.0/3) / float(2.0/3))\n",
    "uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the training set into two groups for training: \n",
    "# A 'steady' group for when interest rates remain steady \n",
    "# A 'move' group for when interest rates move (increase or decrease)\n",
    "\n",
    "training_steady = training[training.Same==1]\n",
    "training_move = training[training.Same==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open training files for each training group and combine into single text files\n",
    "# Feed python the appropriate character set for decoding \n",
    "\n",
    "raw_steady = str()\n",
    "for i in (training_steady.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read()\n",
    "    raw_steady += temp\n",
    "raw_steady = raw_steady.decode('utf_16')\n",
    "\n",
    "raw_move = str()\n",
    "for i in (training_move.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read()\n",
    "    raw_move += temp\n",
    "raw_move = raw_move.decode('utf_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate the individual words in the documents\n",
    "\n",
    "tokens_steady = nltk.word_tokenize(raw_steady)\n",
    "tokens_move = nltk.word_tokenize(raw_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make all words lower case\n",
    "# Remove punctuation, \\n's, and numbers \n",
    "\n",
    "#tokens_steady = [word.lower() for word in tokens_steady if word.isalpha() and len(word)>2]\n",
    "#tokens_move = [word.lower() for word in tokens_move if word.isalpha() and len(word)>2]\n",
    "tokens_steady = [word.lower() for word in tokens_steady if word.isalpha()]\n",
    "tokens_move = [word.lower() for word in tokens_move if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stopwards\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens_steady = [word for word in tokens_steady if word not in stopwords]\n",
    "tokens_move = [word for word in tokens_move if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each word \n",
    "\n",
    "fdist_steady = nltk.FreqDist(tokens_steady)\n",
    "fdist_move = nltk.FreqDist(tokens_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an index for the data table \n",
    "\n",
    "vocabulary_steady = fdist_steady.keys()\n",
    "vocabulary_move = fdist_move.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a data table for the word count \n",
    "\n",
    "df_steady = pd.Series(fdist_steady, index=vocabulary_steady)\n",
    "df_move = pd.Series(fdist_move, index=vocabulary_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add one to every count to deal with rare words via \"additive smoothing\"\n",
    "\n",
    "df_steady = df_steady + 1 \n",
    "df_move = df_move + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide both data tables by the largest frequency value to account for differing sample sizes \n",
    "\n",
    "df_steady = np.log(df_steady / float(df_steady.max()))\n",
    "df_move = np.log(df_move / float(df_move.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the sample probability for each word \n",
    "\n",
    "df_steady = df_steady / sum(df_steady)\n",
    "df_move = df_move / sum(df_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log the values to account for floating point underflow \n",
    "\n",
    "df_steady = np.log(df_steady)\n",
    "df_move = np.log(df_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.describe of economic             -inf\n",
       "inflation      -15.743387\n",
       "growth         -12.046265\n",
       "indonesia      -11.852366\n",
       "bank           -10.770829\n",
       "global         -10.372338\n",
       "financial       -9.999111\n",
       "policy          -9.929142\n",
       "rate            -9.835534\n",
       "rupiah          -9.805059\n",
       "domestic        -9.784922\n",
       "economy         -9.710555\n",
       "capital         -9.686126\n",
       "prices          -9.604173\n",
       "yoy             -9.585103\n",
       "government      -9.490657\n",
       "account         -9.429826\n",
       "market          -9.415823\n",
       "well            -9.392501\n",
       "also            -9.387838\n",
       "expected        -9.289814\n",
       "line            -9.271075\n",
       "investment      -9.242892\n",
       "credit          -9.214594\n",
       "foreign         -9.205132\n",
       "us              -9.200394\n",
       "imports         -9.195653\n",
       "performance     -9.195653\n",
       "stability       -9.176640\n",
       "continue        -9.162328\n",
       "                  ...    \n",
       "disburse        -7.639981\n",
       "downwards       -7.639981\n",
       "encompassing    -7.639981\n",
       "bolsterd        -7.639981\n",
       "rent            -7.639981\n",
       "locations       -7.639981\n",
       "published       -7.639981\n",
       "optimise        -7.639981\n",
       "par             -7.639981\n",
       "maturity        -7.639981\n",
       "uk              -7.639981\n",
       "faded           -7.639981\n",
       "population      -7.639981\n",
       "perpetuation    -7.639981\n",
       "surges          -7.639981\n",
       "appreciates     -7.639981\n",
       "expedition      -7.639981\n",
       "steeped         -7.639981\n",
       "allayed         -7.639981\n",
       "stifled         -7.639981\n",
       "cooling         -7.639981\n",
       "thirdly         -7.639981\n",
       "exists          -7.639981\n",
       "corporations    -7.639981\n",
       "rains           -7.639981\n",
       "intensifying    -7.639981\n",
       "application     -7.639981\n",
       "overheating     -7.639981\n",
       "seasonality     -7.639981\n",
       "optimizing      -7.639981\n",
       "dtype: float64>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the training sets (1)\n",
    "\n",
    "df_steady.sort_values(inplace=True)\n",
    "df_steady.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.describe of economic              -inf\n",
       "indonesia       -11.111224\n",
       "bank            -10.647527\n",
       "inflation       -10.574062\n",
       "growth          -10.210022\n",
       "global          -10.062205\n",
       "financial        -9.213181\n",
       "rate             -9.186834\n",
       "economy          -9.160819\n",
       "domestic         -8.986646\n",
       "policy           -8.939004\n",
       "capital          -8.710373\n",
       "banking          -8.688136\n",
       "prices           -8.643876\n",
       "exchange         -8.621832\n",
       "rupiah           -8.621832\n",
       "pressure         -8.599832\n",
       "credit           -8.534000\n",
       "also             -8.512078\n",
       "stability        -8.468203\n",
       "monetary         -8.446229\n",
       "government       -8.402148\n",
       "market           -8.335511\n",
       "system           -8.335511\n",
       "yoy              -8.313108\n",
       "line             -8.267928\n",
       "performance      -8.267928\n",
       "trend            -8.267928\n",
       "inflationary     -8.245119\n",
       "level            -8.245119\n",
       "                   ...    \n",
       "booked           -7.108022\n",
       "develop          -7.108022\n",
       "intervention     -7.108022\n",
       "mean             -7.108022\n",
       "fasting          -7.108022\n",
       "tracking         -7.108022\n",
       "severe           -7.108022\n",
       "without          -7.108022\n",
       "components       -7.108022\n",
       "persistently     -7.108022\n",
       "adjustments      -7.108022\n",
       "except           -7.108022\n",
       "instrument       -7.108022\n",
       "stagnant         -7.108022\n",
       "around           -7.108022\n",
       "road             -7.108022\n",
       "annually         -7.108022\n",
       "driving          -7.108022\n",
       "swaps            -7.108022\n",
       "confirm          -7.108022\n",
       "lend             -7.108022\n",
       "necessitating    -7.108022\n",
       "freed            -7.108022\n",
       "comparison       -7.108022\n",
       "package          -7.108022\n",
       "reinforcing      -7.108022\n",
       "stand            -7.108022\n",
       "act              -7.108022\n",
       "output           -7.108022\n",
       "rural            -7.108022\n",
       "dtype: float64>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the training sets (2)\n",
    "\n",
    "df_move.sort_values(inplace=True)\n",
    "df_move.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After doc 25 interest rates should move.\n",
      "After doc 59 interest rates should move.\n",
      "After doc 67 interest rates should move.\n",
      "After doc 6 interest rates should move.\n",
      "After doc 14 interest rates should move.\n",
      "After doc 30 interest rates should move.\n",
      "After doc 20 interest rates should move.\n",
      "After doc 1 interest rates should move.\n",
      "After doc 15 interest rates should move.\n",
      "After doc 72 interest rates should move.\n",
      "After doc 50 interest rates should move.\n",
      "After doc 21 interest rates should move.\n",
      "After doc 32 interest rates should move.\n",
      "After doc 11 interest rates should move.\n",
      "After doc 70 interest rates should move.\n",
      "After doc 48 interest rates should move.\n",
      "After doc 19 interest rates should move.\n"
     ]
    }
   ],
   "source": [
    "# Open files for the testing group and combine into a single text file\n",
    "# Feed python the appropriate character set for decoding \n",
    "# Separate the individual words in the testing document\n",
    "# Make all words lower case\n",
    "# Remove punctuation, \\n's, numbers, and words with less than 3 letters\n",
    "# Create a data frame for the testing set \n",
    "# Calculate probabilities for test set from training sets \n",
    "\n",
    "for i in (testing.index + 1):\n",
    "    fname = 'C:/Users/ajarczyk/Dropbox/Programming/BI_Project/BIEngStatement' + str(i) + '.txt'\n",
    "    temp = open(fname).read().decode('utf_16')\n",
    "    tokens = nltk.word_tokenize(temp)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha() and len(word)>2]\n",
    "    df_testing = pd.Series(tokens)    \n",
    "    p_steady = df_testing.map(df_steady).sum() \n",
    "    p_move = df_testing.map(df_move).sum()\n",
    "    #print \"P-Steady for doc \" + str(i) + \" is\", p_steady\n",
    "    #print \"P-Move for doc \" + str(i) + \" is\", p_move\n",
    "    if (p_steady - p_move) > 0: \n",
    "        print \"After doc \" + str(i) + \" interest rates should be steady.\" \n",
    "    else:\n",
    "        print \"After doc \" + str(i) + \" interest rates should move.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
